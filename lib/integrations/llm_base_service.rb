class Integrations::LlmBaseService
  include Integrations::LlmInstrumentation

  # gpt-4o-mini supports 128,000 tokens
  # 1 token is approx 4 characters
  # sticking with 120000 to be safe
  # 120000 * 4 = 480,000 characters (rounding off downwards to 400,000 to be safe)
  TOKEN_LIMIT = 400_000
  GPT_MODEL = Llm::Config::DEFAULT_MODEL
  ALLOWED_EVENT_NAMES = %w[rephrase summarize reply_suggestion fix_spelling_grammar shorten expand make_friendly make_formal simplify].freeze
  CACHEABLE_EVENTS = %w[].freeze

  pattr_initialize [:hook!, :event!]

  def perform
    return nil unless valid_event_name?

    return value_from_cache if value_from_cache.present?

    response = send("#{event_name}_message")
    save_to_cache(response) if response.present?

    response
  end

  private

  def event_name
    event['name']
  end

  def cache_key
    return nil unless event_is_cacheable?

    return nil unless conversation

    # since the value from cache depends on the conversation last_activity_at, it will always be fresh
    format(::Redis::Alfred::OPENAI_CONVERSATION_KEY, event_name: event_name, conversation_id: conversation.id,
                                                     updated_at: conversation.last_activity_at.to_i)
  end

  def value_from_cache
    return nil unless event_is_cacheable?
    return nil if cache_key.blank?

    deserialize_cached_value(Redis::Alfred.get(cache_key))
  end

  def deserialize_cached_value(value)
    return nil if value.blank?

    JSON.parse(value, symbolize_names: true)
  rescue JSON::ParserError
    # If json parse failed, returning the value as is will fail too
    # since we access the keys as symbols down the line
    # So it's best to return nil
    nil
  end

  def save_to_cache(response)
    return nil unless event_is_cacheable?

    # Serialize to JSON
    # This makes parsing easy when response is a hash
    Redis::Alfred.setex(cache_key, response.to_json)
  end

  def conversation
    @conversation ||= hook.account.conversations.find_by(display_id: event['data']['conversation_display_id'])
  end

  def valid_event_name?
    # self.class::ALLOWED_EVENT_NAMES is way to access ALLOWED_EVENT_NAMES defined in the class hierarchy of the current object.
    # This ensures that if ALLOWED_EVENT_NAMES is updated elsewhere in it's ancestors, we access the latest value.
    self.class::ALLOWED_EVENT_NAMES.include?(event_name)
  end

  def event_is_cacheable?
    # self.class::CACHEABLE_EVENTS is way to access CACHEABLE_EVENTS defined in the class hierarchy of the current object.
    # This ensures that if CACHEABLE_EVENTS is updated elsewhere in it's ancestors, we access the latest value.
    self.class::CACHEABLE_EVENTS.include?(event_name)
  end

  def api_base
    endpoint = hook.settings['api_base_url'].presence || InstallationConfig.find_by(name: 'CAPTAIN_OPEN_AI_ENDPOINT')&.value.presence || 'https://api.openai.com/'
    endpoint = endpoint.chomp('/')
    endpoint.end_with?('/v1') ? endpoint : "#{endpoint}/v1"
  end

  def make_api_call(body)
    parsed_body = JSON.parse(body)
    instrumentation_params = build_instrumentation_params(parsed_body)

    instrument_llm_call(instrumentation_params) do
      execute_ruby_llm_request(parsed_body)
    end
  end

  def execute_ruby_llm_request(parsed_body)
    messages = parsed_body['messages']
    
    # Utiliza o modelo configurado no hook ou faz fallback o modelo solicitado / padrao
    configured_model = hook.settings['model_name'].presence || parsed_body['model'] || GPT_MODEL
    # Limpa prefixos de provedores que algumas listagens retornam (ex: groq/llama3 -> llama3)
    configured_model = configured_model.split('/').last if configured_model.to_s.include?('/')

    # Se há uma URL customizada (não é api.openai.com), usar chamada HTTP direta
    # para evitar a validação de modelos da gem RubyLLM
    custom_url = hook.settings['api_base_url'].presence
    if custom_url.present?
      execute_direct_http_request(configured_model, messages)
    else
      Llm::Config.with_api_key(hook.settings['api_key'], api_base: api_base) do |context|
        chat = context.chat(model: configured_model)
        setup_chat_with_messages(chat, messages)
      end
    end
  rescue StandardError => e
    ChatwootExceptionTracker.new(e, account: hook.account).capture_exception
    build_error_response_from_exception(e, messages)
  end

  # Chamada direta via Net::HTTP para provedores OpenAI-compatible (Groq, Maritaca, Ollama, etc.)
  def execute_direct_http_request(model, messages)
    require 'net/http'
    require 'uri'
    require 'json'

    chat_url = "#{api_base}/chat/completions"
    uri = URI.parse(chat_url)

    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = (uri.scheme == 'https')
    http.open_timeout = 30
    http.read_timeout = 60

    request = Net::HTTP::Post.new(uri.request_uri)
    request['Authorization'] = "Bearer #{hook.settings['api_key']}"
    request['Content-Type'] = 'application/json'

    body = {
      model: model,
      messages: messages
    }

    request.body = body.to_json
    response = http.request(request)

    if response.code.to_i == 200
      parsed = JSON.parse(response.body)
      content = parsed.dig('choices', 0, 'message', 'content') || ''
      {
        message: content,
        usage: parsed['usage'] || {},
        request_messages: messages
      }
    else
      error_body = JSON.parse(response.body) rescue { 'error' => response.body }
      error_msg = error_body.dig('error', 'message') || error_body['error'] || "HTTP #{response.code}"
      { error: error_msg, request_messages: messages }
    end
  end

  def setup_chat_with_messages(chat, messages)
    apply_system_instructions(chat, messages)
    response = send_conversation_messages(chat, messages)
    return { error: 'No conversation messages provided', error_code: 400, request_messages: messages } if response.nil?

    build_ruby_llm_response(response, messages)
  end

  def apply_system_instructions(chat, messages)
    system_msg = messages.find { |m| m['role'] == 'system' }
    chat.with_instructions(system_msg['content']) if system_msg
  end

  def send_conversation_messages(chat, messages)
    conversation_messages = messages.reject { |m| m['role'] == 'system' }

    return nil if conversation_messages.empty?

    return chat.ask(conversation_messages.first['content']) if conversation_messages.length == 1

    add_conversation_history(chat, conversation_messages[0...-1])
    chat.ask(conversation_messages.last['content'])
  end

  def add_conversation_history(chat, messages)
    messages.each do |msg|
      chat.add_message(role: msg['role'].to_sym, content: msg['content'])
    end
  end

  def build_ruby_llm_response(response, messages)
    {
      message: response.content,
      usage: {
        'prompt_tokens' => response.input_tokens,
        'completion_tokens' => response.output_tokens,
        'total_tokens' => (response.input_tokens || 0) + (response.output_tokens || 0)
      },
      request_messages: messages
    }
  end

  def build_instrumentation_params(parsed_body)
    {
      span_name: "llm.#{event_name}",
      account_id: hook.account_id,
      conversation_id: conversation&.display_id,
      feature_name: event_name,
      model: parsed_body['model'],
      messages: parsed_body['messages'],
      temperature: parsed_body['temperature']
    }
  end

  def build_error_response_from_exception(error, messages)
    { error: error.message, request_messages: messages }
  end
end
